import{S as zi,i as xi,s as Ki,k as r,q as l,a as h,l as o,m as n,r as f,h as i,c as v,n as ei,b as a,H as s,G as ii}from"../chunks/index.12bdb194.js";function Ri(Li){let y,we,B,_,U,_e,D,u,Ee,b,ke,Ie,J,E,ge,V,p,F,qe,Pe,N,Te,He,W,Ae,Y,k,Le,Z,I,ze,$,g,xe,ee,q,Ke,ie,c,X,Re,Me,C,Se,te,P,Ue,se,T,Fe,ae,H,Ne,re,d,G,We,Xe,O,Ce,oe,w,Ge,ne,A,Oe,le,L,Qe,fe,z,je,he,x,Be,ve,K,De,pe,m,Q,Je,Ve,j,Ye,ye,R,Ze,ue,M,$e;return{c(){y=r("h1"),we=l("Roles for third parties in promoting safe AI innovation"),B=h(),_=r("p"),U=r("em"),_e=l("This is a summary of a paper I have been working on with Robert Trager and Nicholas Emery-Xu. Feel free to reach out for the full version."),D=h(),u=r("p"),Ee=l("This paper builds on "),b=r("a"),ke=l("our previous work"),Ie=l(`, where
we introduced a model where innovators compete to be the first to
develop advanced AI technology, allocating resources between safety and
performance research. In that paper, we made some observations on how
changes in compute pricing affect safety. Here, we suggest and evaluate
a few other ways that third parties could promote safer strategies on
the part of AI innovators.`),J=h(),E=r("p"),ge=l("The basic interventions we look at are:"),V=h(),p=r("ol"),F=r("li"),qe=l("Providing subsidies that can be used only for safety research"),Pe=h(),N=r("li"),Te=l("Providing subsidies only if innovators can certify they will meet a threshold level of safety"),He=h(),W=r("li"),Ae=l("Penalizing innovators if they fail to meet a threshold level of safety"),Y=h(),k=r("p"),Le=l(`Roughly speaking, all of these interventions are better at promoting
safety than simply providing compute subsidies to be used at innovators’
discretion. The main takeaway is that interventions 2 and 3 can be significantly more effective (both in terms of cost and the resulting increase in safety) than intervention 1.`),Z=h(),I=r("h2"),ze=l("Targeted subsidies for safety spending"),$=h(),g=r("p"),xe=l(`Here, we consider the case where innovators receive discounted prices
for inputs to their safety research programs (and not to any programs
meant to improve performance/capabilities).`),ee=h(),q=r("p"),Ke=l("Key takeaways:"),ie=h(),c=r("ul"),X=r("li"),Re=l("If all innovators are roughly identical, subsidizing everyone’s safety spending always increases equilibrium safety."),Me=h(),C=r("li"),Se=l("If innovators are not identical, or we offer different subsidies to different innovators, this intervention may not be as effective, or may be counterproductive, but it at least seems difficult to actively do harm."),te=h(),P=r("h2"),Ue=l("Subsidies contingent on safety certification"),se=h(),T=r("p"),Fe=l(`Here, we consider the case where a third party audits innovators’
spending plans; if their plans fulfill some threshold safety
requirement, they are certified safe and receive a subsidy on all their
spending.`),ae=h(),H=r("p"),Ne=l("Key takeaways:"),re=h(),d=r("ul"),G=r("li"),We=l("If the threshold safety requirement is too low or too high, this policy is ineffective (innovators will either qualify for the subsidy without changing their behavior or not bother qualifying since the requirements are too difficult to fulfill)."),Xe=h(),O=r("li"),Ce=l("If the threshold safety requirement is chosen appropriately (as high as possible while still being attractive to fulfill), this can be a very effective intervention."),oe=h(),w=r("h3"),Ge=l("An important clarification"),ne=l(`
This intervention is not the same as identifying actors who are
deemed to be `),A=r("q"),Oe=l("safer"),le=l(` than others and offering them subsidies: the
key difference is that here the subsidy is based on the level of safety
that will be achieved `),L=r("em"),Qe=l("after the fact"),fe=l(`. That is, innovators only
qualify if they will achieve a safety target after being given the
subsidy. This avoids the potential problem of actors who are relatively
safe before being subsidized becoming less safe after being subsidized,
or less safety-conscious actors not selected for subsidies behaving
recklessly in reaction to their safety-conscious competitors recieving
subsidies.
`),z=r("h2"),je=l("Penalties for safety violations"),he=h(),x=r("p"),Be=l(`Here, we again consider a case where a third party audits innovators
and certifies them as safe (or not). However, in this case, there is no
subsidy for passing the audit; instead, innovators who fail are
penalized.`),ve=h(),K=r("p"),De=l("Key takeaways:"),pe=h(),m=r("ul"),Q=r("li"),Je=l("This exhibits similar characteristics to the previous intervention (where certified innovators receive subsidies rather than escaping penalties)."),Ve=h(),j=r("li"),Ye=l("Ideally, in equilibrium, nobody pays the penalties, and all innovators act safely at no cost to third parties."),ye=h(),R=r("h3"),Ze=l("Why would innovators agree to this?"),ue=h(),M=r("p"),$e=l(`An AI race may behave like a prisoner’s dilemma, where everyone would
prefer to adopt safer strategies but won’t since defecting (taking a
riskier, high-performance, strategy) while everyone else is playing safe
destabilizes the equilibrium. Making defection more costly may therefore
be desirable to everyone if it makes the case where everyone plays
safely a stable equilibrium. (Innovators may actually prefer more severe
penalties if that is what it takes to ensure that their competitors
won’t defect.)`),this.h()},l(e){y=o(e,"H1",{});var t=n(y);we=f(t,"Roles for third parties in promoting safe AI innovation"),t.forEach(i),B=v(e),_=o(e,"P",{});var ti=n(_);U=o(ti,"EM",{});var si=n(U);_e=f(si,"This is a summary of a paper I have been working on with Robert Trager and Nicholas Emery-Xu. Feel free to reach out for the full version."),si.forEach(i),ti.forEach(i),D=v(e),u=o(e,"P",{});var ce=n(u);Ee=f(ce,"This paper builds on "),b=o(ce,"A",{href:!0,rel:!0});var ai=n(b);ke=f(ai,"our previous work"),ai.forEach(i),Ie=f(ce,`, where
we introduced a model where innovators compete to be the first to
develop advanced AI technology, allocating resources between safety and
performance research. In that paper, we made some observations on how
changes in compute pricing affect safety. Here, we suggest and evaluate
a few other ways that third parties could promote safer strategies on
the part of AI innovators.`),ce.forEach(i),J=v(e),E=o(e,"P",{});var ri=n(E);ge=f(ri,"The basic interventions we look at are:"),ri.forEach(i),V=v(e),p=o(e,"OL",{});var S=n(p);F=o(S,"LI",{});var oi=n(F);qe=f(oi,"Providing subsidies that can be used only for safety research"),oi.forEach(i),Pe=v(S),N=o(S,"LI",{});var ni=n(N);Te=f(ni,"Providing subsidies only if innovators can certify they will meet a threshold level of safety"),ni.forEach(i),He=v(S),W=o(S,"LI",{});var li=n(W);Ae=f(li,"Penalizing innovators if they fail to meet a threshold level of safety"),li.forEach(i),S.forEach(i),Y=v(e),k=o(e,"P",{});var fi=n(k);Le=f(fi,`Roughly speaking, all of these interventions are better at promoting
safety than simply providing compute subsidies to be used at innovators’
discretion. The main takeaway is that interventions 2 and 3 can be significantly more effective (both in terms of cost and the resulting increase in safety) than intervention 1.`),fi.forEach(i),Z=v(e),I=o(e,"H2",{});var hi=n(I);ze=f(hi,"Targeted subsidies for safety spending"),hi.forEach(i),$=v(e),g=o(e,"P",{});var vi=n(g);xe=f(vi,`Here, we consider the case where innovators receive discounted prices
for inputs to their safety research programs (and not to any programs
meant to improve performance/capabilities).`),vi.forEach(i),ee=v(e),q=o(e,"P",{});var pi=n(q);Ke=f(pi,"Key takeaways:"),pi.forEach(i),ie=v(e),c=o(e,"UL",{});var de=n(c);X=o(de,"LI",{});var yi=n(X);Re=f(yi,"If all innovators are roughly identical, subsidizing everyone’s safety spending always increases equilibrium safety."),yi.forEach(i),Me=v(de),C=o(de,"LI",{});var ui=n(C);Se=f(ui,"If innovators are not identical, or we offer different subsidies to different innovators, this intervention may not be as effective, or may be counterproductive, but it at least seems difficult to actively do harm."),ui.forEach(i),de.forEach(i),te=v(e),P=o(e,"H2",{});var ci=n(P);Ue=f(ci,"Subsidies contingent on safety certification"),ci.forEach(i),se=v(e),T=o(e,"P",{});var di=n(T);Fe=f(di,`Here, we consider the case where a third party audits innovators’
spending plans; if their plans fulfill some threshold safety
requirement, they are certified safe and receive a subsidy on all their
spending.`),di.forEach(i),ae=v(e),H=o(e,"P",{});var mi=n(H);Ne=f(mi,"Key takeaways:"),mi.forEach(i),re=v(e),d=o(e,"UL",{});var me=n(d);G=o(me,"LI",{});var bi=n(G);We=f(bi,"If the threshold safety requirement is too low or too high, this policy is ineffective (innovators will either qualify for the subsidy without changing their behavior or not bother qualifying since the requirements are too difficult to fulfill)."),bi.forEach(i),Xe=v(me),O=o(me,"LI",{});var wi=n(O);Ce=f(wi,"If the threshold safety requirement is chosen appropriately (as high as possible while still being attractive to fulfill), this can be a very effective intervention."),wi.forEach(i),me.forEach(i),oe=v(e),w=o(e,"H3",{id:!0});var _i=n(w);Ge=f(_i,"An important clarification"),_i.forEach(i),ne=f(e,`
This intervention is not the same as identifying actors who are
deemed to be `),A=o(e,"Q",{});var Ei=n(A);Oe=f(Ei,"safer"),Ei.forEach(i),le=f(e,` than others and offering them subsidies: the
key difference is that here the subsidy is based on the level of safety
that will be achieved `),L=o(e,"EM",{});var ki=n(L);Qe=f(ki,"after the fact"),ki.forEach(i),fe=f(e,`. That is, innovators only
qualify if they will achieve a safety target after being given the
subsidy. This avoids the potential problem of actors who are relatively
safe before being subsidized becoming less safe after being subsidized,
or less safety-conscious actors not selected for subsidies behaving
recklessly in reaction to their safety-conscious competitors recieving
subsidies.
`),z=o(e,"H2",{});var Ii=n(z);je=f(Ii,"Penalties for safety violations"),Ii.forEach(i),he=v(e),x=o(e,"P",{});var gi=n(x);Be=f(gi,`Here, we again consider a case where a third party audits innovators
and certifies them as safe (or not). However, in this case, there is no
subsidy for passing the audit; instead, innovators who fail are
penalized.`),gi.forEach(i),ve=v(e),K=o(e,"P",{});var qi=n(K);De=f(qi,"Key takeaways:"),qi.forEach(i),pe=v(e),m=o(e,"UL",{});var be=n(m);Q=o(be,"LI",{});var Pi=n(Q);Je=f(Pi,"This exhibits similar characteristics to the previous intervention (where certified innovators receive subsidies rather than escaping penalties)."),Pi.forEach(i),Ve=v(be),j=o(be,"LI",{});var Ti=n(j);Ye=f(Ti,"Ideally, in equilibrium, nobody pays the penalties, and all innovators act safely at no cost to third parties."),Ti.forEach(i),be.forEach(i),ye=v(e),R=o(e,"H3",{});var Hi=n(R);Ze=f(Hi,"Why would innovators agree to this?"),Hi.forEach(i),ue=v(e),M=o(e,"P",{});var Ai=n(M);$e=f(Ai,`An AI race may behave like a prisoner’s dilemma, where everyone would
prefer to adopt safer strategies but won’t since defecting (taking a
riskier, high-performance, strategy) while everyone else is playing safe
destabilizes the equilibrium. Making defection more costly may therefore
be desirable to everyone if it makes the case where everyone plays
safely a stable equilibrium. (Innovators may actually prefer more severe
penalties if that is what it takes to ensure that their competitors
won’t defect.)`),Ai.forEach(i),this.h()},h(){ei(b,"href","https://arxiv.org/pdf/2302.11436.pdf"),ei(b,"rel","nofollow"),ei(w,"id","an-important-clarification")},m(e,t){a(e,y,t),s(y,we),a(e,B,t),a(e,_,t),s(_,U),s(U,_e),a(e,D,t),a(e,u,t),s(u,Ee),s(u,b),s(b,ke),s(u,Ie),a(e,J,t),a(e,E,t),s(E,ge),a(e,V,t),a(e,p,t),s(p,F),s(F,qe),s(p,Pe),s(p,N),s(N,Te),s(p,He),s(p,W),s(W,Ae),a(e,Y,t),a(e,k,t),s(k,Le),a(e,Z,t),a(e,I,t),s(I,ze),a(e,$,t),a(e,g,t),s(g,xe),a(e,ee,t),a(e,q,t),s(q,Ke),a(e,ie,t),a(e,c,t),s(c,X),s(X,Re),s(c,Me),s(c,C),s(C,Se),a(e,te,t),a(e,P,t),s(P,Ue),a(e,se,t),a(e,T,t),s(T,Fe),a(e,ae,t),a(e,H,t),s(H,Ne),a(e,re,t),a(e,d,t),s(d,G),s(G,We),s(d,Xe),s(d,O),s(O,Ce),a(e,oe,t),a(e,w,t),s(w,Ge),a(e,ne,t),a(e,A,t),s(A,Oe),a(e,le,t),a(e,L,t),s(L,Qe),a(e,fe,t),a(e,z,t),s(z,je),a(e,he,t),a(e,x,t),s(x,Be),a(e,ve,t),a(e,K,t),s(K,De),a(e,pe,t),a(e,m,t),s(m,Q),s(Q,Je),s(m,Ve),s(m,j),s(j,Ye),a(e,ye,t),a(e,R,t),s(R,Ze),a(e,ue,t),a(e,M,t),s(M,$e)},p:ii,i:ii,o:ii,d(e){e&&i(y),e&&i(B),e&&i(_),e&&i(D),e&&i(u),e&&i(J),e&&i(E),e&&i(V),e&&i(p),e&&i(Y),e&&i(k),e&&i(Z),e&&i(I),e&&i($),e&&i(g),e&&i(ee),e&&i(q),e&&i(ie),e&&i(c),e&&i(te),e&&i(P),e&&i(se),e&&i(T),e&&i(ae),e&&i(H),e&&i(re),e&&i(d),e&&i(oe),e&&i(w),e&&i(ne),e&&i(A),e&&i(le),e&&i(L),e&&i(fe),e&&i(z),e&&i(he),e&&i(x),e&&i(ve),e&&i(K),e&&i(pe),e&&i(m),e&&i(ye),e&&i(R),e&&i(ue),e&&i(M)}}}class Si extends zi{constructor(y){super(),xi(this,y,null,Ri,Ki,{})}}export{Si as default};
