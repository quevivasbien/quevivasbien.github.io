import{S as _i,i as Wi,s as Mi,k as n,q as a,a as m,l as s,m as l,r as i,h as t,c as u,n as p,p as Gi,T as Ti,b as h,H as o,C as xa}from"../chunks/index.d8bf866d.js";function qi(xi){let _,qt,Xe,W,Pt,D,jt,At,ze,X,z,se,Lt,Ht,Fe,F,le,Ot,Ue,U,re,Bt,Ce,M,St,he,Yt,$t,Ke,B,Nt,Qe,x,Dt,me,Xt,zt,ue,Je,f,Ft,ce,Ut,Ct,G,_a,Kt,de,Qt,Jt,Re,c,Rt,fe,Vt,Zt,ye,eo,to,pe,oo,ao,we,io,no,Ve,T,so,ge,lo,ro,Ze,C,ho,et,q,mo,K,uo,co,tt,Q,fo,ot,S,yo,at,J,po,it,R,wo,nt,w,go,be,bo,vo,ve,Io,ko,st,g,Eo,Ie,xo,_o,ke,Wo,Mo,lt,d,Go,Ee,To,qo,xe,Po,jo,_e,Ao,Lo,We,Ho,Oo,rt,P,Bo,Me,So,Yo,ht,j,$o,Ge,No,Do,mt,Y,Xo,ut,b,zo,Te,Fo,Uo,qe,Co,Ko,ct,v,Qo,Pe,Jo,Ro,je,Vo,Zo,dt,$,ea,ft,y,ta,Ae,oa,aa,Le,ia,na,He,sa,la,yt,A,ra,Oe,ha,ma,pt,V,ua,wt,L,Be,ca,da,Se,fa,gt,Z,ya,bt,N,pa,vt,ee,wa,It,I,Ye,ga,ba,$e,va,Ia,Ne,ka;return{c(){_=n("h1"),qt=a("Diving deeper with Axiom M"),Xe=m(),W=n("p"),Pt=a("I "),D=n("a"),jt=a("earlier"),At=a(` made some
    rather dubitable statements that warrant a bit more careful consideration. Here’s the relevant
    text:`),ze=m(),X=n("blockquote"),z=n("p"),se=n("em"),Lt=a("People ought to act in a way that will get them what they want."),Ht=a("…"),Fe=m(),F=n("blockquote"),le=n("p"),Ot=a(`Note that I am not suggesting that we should blindly follow whatever whims flutter across our
        minds. What I am suggesting is that we should endeavor to put ourselves in states that we
        prefer to all other feasible options. Giving precedence to transitory wants is a bad
        strategy for Getting What You Want since those desires are likely to change quickly, making
        them difficult to satisfy in any meaningful way. (As a general maxim, an important part of
        the project of Getting What You Want is wanting the right things in the first place.)`),Ue=m(),U=n("blockquote"),re=n("p"),Bt=a(`…it’s helpful to have desires that are actually achievable. It may even be worthwhile to
        intentionally train yourself to desire things that are more feasible.`),Ce=m(),M=n("p"),St=a(`For sake of notational simplicity, let’s call the statement “I should act in a way that gets me
    what I want” `),he=n("em"),Yt=a("axiom M"),$t=a(` (’cause it’s Mckay’s Moral axiom). In the rationalist manifesto, I
    made the claim that axiom M is the only sensible moral axiom. That was perhaps too strong of a
    statement – I’m open to the idea that there may be other moral axioms that I could get behind; I
    just haven’t encountered any yet. However, it’s clear from the quotations I’ve included here
    that there are some details about the deployment of this axiom that need to be sorted out.`),Ke=m(),B=n("h2"),Nt=a("Why this axiom?"),Qe=m(),x=n("p"),Dt=a("Before diving into the weeds, I want to take a bit of space to explain "),me=n("em"),Xt=a("why"),zt=a(` I like axiom
    M.`),ue=n("span"),Je=m(),f=n("p"),Ft=a(`Typically, when people think about morality, what they have in mind is some sort of universal
    principle or set of laws that govern what we `),ce=n("em"),Ut=a("should"),Ct=a(` do. For a lot of people, this moral
    law or principle usually comes with some sort of divine authority; that is, there is some sort
    of godlike force or being that dictates what it means for something to be right or wrong.
    (Typically, X is good `),G=n("img"),Kt=a(` God says
    X is good.) The problem that I see with this way of thinking is that it’s not clear to me what
    force these moral laws are actually supposed to have: Sure, maybe God says that I shouldn’t do
    X, but what if I just don’t care? Oh, you say that God will punish me if I do X? Well, what if I
    don’t care about the punishment? What if being able to do X now is more important to me than the
    long-term consequence? What if my nature is such that I would actually prefer hell to heaven? If
    that is the case, saying that I should still do X doesn’t seem to make much practical sense.
    Now, depending on your theology, you might have responses to these objections, but the point
    remains that I don’t `),de=n("em"),Qt=a("have"),Jt=a(` to care, and that point is enough to spur me to look a little
    bit deeper for a principle to base my idea of morality upon. I’ve focused here on the idea that
    moral laws come from some sort of God entity, but the objections I have can be applied even
    (especially!) if you do not believe in a God in the traditional sense. I’ve focused on the
    “divine command” argument simply because it’s been my observation that people who hold that
    perspective tend to have the most vociferous objections to the kind of relativistic thinking
    entailed by something like axiom M, and I don’t think it should be relevant.`),Re=m(),c=n("p"),Rt=a(`Let’s jump back a little bit and ask ourselves why the idea of morality should have any force at
    all? Well, if you believe that morality comes from God, and that we should be good in order to
    receive God’s blessings and avoid God’s punishments, then what you are implicitly admitting is
    that your idea of morality has force because you `),fe=n("em"),Vt=a("want"),Zt=a(` the positive consequences that
    God can impose and `),ye=n("em"),eo=a("don’t want"),to=a(` the negative consequences that God can impose. If you
    believe that morality is simply a feature of objective reality (which, as an aside, I personally
    have a really hard time wrapping my head around, but if you have a good argument for this, I’d
    be happy to listen), then again the only reason why you would pay any attention to that
    particular feature of reality is if some part of you `),pe=n("em"),oo=a("wants"),ao=a(` to take it seriously. And of
    course, if you think that morality is simply an emergent property of the human psyche, then
    ideas about morality can, without much difficulty, be conceptualized as you `),we=n("em"),io=a("wanting"),no=a(`
    things that you consider to be “good.”`),Ve=m(),T=n("p"),so=a(`It seems like we should take seriously the idea that our fascination with morality is based upon
    the underlying fact that, well, we `),ge=n("em"),lo=a("want"),ro=a(` things. We have preferences: we have some idea
    of “good” and “bad,” and we want things that are good and don’t want things that are bad.`),Ze=m(),C=n("p"),ho=a(`I hope I’ve made a good enough case that you can at least see where I’m coming from when I say
    that axiom M is sensible. Note that I am deliberate in my choice of the word “axiom.” I do not
    consider this to be an incontrovertible law of nature, or anything of the sort. It is merely a
    convenient place to start, a postulate that seems to take me in useful directions.`),et=m(),q=n("p"),mo=a(`One last note before moving on: The thinking that leads me to axiom M is very similar to the
    thinking that leads to utilitarian theories of morality. In fact, axiom M and its corollaries
    are essentially a flavor of `),K=n("a"),uo=a(`preference
        utilitarianism`),co=a(`. I note this to show that I am far from the first person to wander down
    these paths of thought (and certainly not the most precise or eloquent expositor of these
    ideas). My intent here is not to reinvent the wheel, rather to suggest how to put these ideas
    into practice and show why thinking in this way is useful.`),tt=m(),Q=n("p"),fo=a("Now, let’s move onward and examine what axiom M actually entails."),ot=m(),S=n("h2"),yo=a("Temporal dependence"),at=m(),J=n("p"),po=a(`For the sake of philosophical exploration, let’s hop inside my mind as I go through the process
    of making a decision.`),it=m(),R=n("p"),wo=a(`Let’s suppose that I’m hungry. The decision in question is what to do about that hunger. To
    answer that question, we need to consider what it is I want in this moment. Well, I’m hungry, so
    of course I want to eat something. If I’m hungry enough, that desire could be so great that it
    monopolizes my mental space, making it difficult to substantially desire anything else. In such
    a case – according to our axiom – the thing to do is to satisfy that desire by finding something
    to eat; anything else would not be Getting What I Want.`),nt=m(),w=n("p"),go=a(`Now let’s add a little bit of context. Let’s suppose that, for some reason, the only easily
    available food is a tub of ice cream. Eating that ice cream will satisfy my present hunger, but,
    in the long run, might have negative consequences on my health. (Granted, eating ice cream once
    is probably not going to have noticeable consequences, but for the sake of argument let’s
    suppose that this particular ice cream is abnormally unhealthy. Maybe it’s sweetened with lead
    or something.) Should the health effect of the ice cream be a consideration in my
    decision-making process? Well, if I am at a point of hunger that I literally desire nothing else
    except to satisfy my hunger, then – according to our axiom – no. If my `),be=n("em"),bo=a("only"),vo=a(` desire is
    to satisfy my hunger, then I `),ve=n("em"),Io=a("don’t care"),ko=a(` about the health effects. If I don’t care about
    the health effects, then why should they influence my decision?`),st=m(),g=n("p"),Eo=a("Under normal circumstances, I "),Ie=n("em"),xo=a("do"),_o=a(` care about the health effects of my actions. In fact,
    if I go ahead and eat the ice cream, then after my hunger goes away I will probably be much more
    concerned about lead poisoning than about my temporary aversion of hunger. Does that mean that
    it was not the right thing to do? Well, in the moment I made the decision, it `),ke=n("em"),Wo=a("was"),Mo=a(` the
    right thing to do, since it was the action that would Get Me What I Wanted. However, after the
    fact, with the desire to not be hungry no longer dominating my consciousness, that decision is
    no longer conducive to what I want. If I could go back in time, retaining the desires I have
    after making the decision the first time, it would no longer be the right thing to do.`),lt=m(),d=n("p"),Go=a(`I should note that my thinking about all this implicitly assumes that desires are highly
    temporally dependent. That is, the desires present in my mind at one moment are likely different
    from those present at another moment. The things I want today are at least slightly different
    from the things I want tomorrow. If you prefer to think that your desires are in some sense more
    permanent, either based on a commitment to a more solid sense of personal identity or to a sense
    that preferences held on a subconscious level should be relevant, you will likely arrive at
    slightly different conclusions that the ones I draw here. In any case, my claim is that only the
    things I want `),Ee=n("em"),To=a("right now"),qo=a(" are relevant to the decision I am making "),xe=n("em"),Po=a("right now"),jo=a(`.
    This does `),_e=n("em"),Ao=a("not"),Lo=a(` mean that only short-term considerations are relevant, like in the ice
    cream example. My health, for example, is a long-term consideration that `),We=n("em"),Ho=a("is"),Oo=a(` relevant in
    a lot of the decisions that I make, but it can only be relevant to a particular decision if I on
    some level actually consciously process that desire while I am making the decision.`),rt=m(),P=n("p"),Bo=a(`So what is the right thing for me to do after eating the ice cream and regretting having done so?
    Well, since at the current moment I `),Me=n("em"),So=a("do"),Yo=a(` want to be healthy, and eating poisonous ice
    cream is inimical to my health, one good course of action is to act in a way that makes it less
    likely that I will eat poisonous ice cream again in the future. That could involve obvious steps
    like getting rid of the ice cream, or going and buying some other food. It could also involve
    intentionally retraining my mind so that when I get really hungry again, the desire to eat will
    not push aside all other desires.`),ht=m(),j=n("p"),$o=a(`One thing I want to point out is that, when making any given decision, short-term desires are not
    inherently more or less important than long-term desires. If my short-term desires are at some
    instant more intense than my long-term desires, I should weight my short-term desires more
    heavily, and the reverse is also true. However, if I am at a juncture where long-term desires
    are relevant, Getting What I Want may involve taking steps to make sure that those long-term
    desires `),Ge=n("em"),No=a("remain"),Do=a(` relevant. (Being ruled by short-term desires may mean that you Get What
    You Want now at the expense of not Getting What You Want later. If all my desires are short
    term, that doesn’t matter, since I don’t care about what happens to me later.) I do think that
    someone who takes axiom M seriously will take deliberate steps to keep long-term desires at the
    forefront of his or her consideration, if only because allowing short-term desires to be too
    prominent will likely frustrate long-term desires when they do show up.`),mt=m(),Y=n("h2"),Xo=a("Spatial dependence"),ut=m(),b=n("p"),zo=a(`Just like I cannot use axiom M to sensibly make claims about I should want in the future (or
    should have wanted in the past) based on my current desires, it is not particularly meaningful
    to employ axiom M for making claims about what other people should or should not be doing given
    `),Te=n("em"),Fo=a("my"),Uo=a(` personal preferences. I can say things like, “I don’t like the way Alice acts,” but
    statements like, “Bob should not act that way,” don’t make much sense given the framework I’ve
    tried to set up here, unless what I am implying is that Bob’s actions are not aligned with
    `),qe=n("em"),Co=a("his"),Ko=a(` preferences. In general, I cannot make statements about what is right for other
    people unless I know what it is they want.`),ct=m(),v=n("p"),Qo=a(`Another point that I should make is that the degree to which other people are Getting What They
    Want is not inherently relevant in my decision making; if I don’t actually care about other
    people, axiom M does not require me to act to benefit them. That said, I want to state
    emphatically that just because the welfare of others is not `),Pe=n("em"),Jo=a("inherently"),Ro=a(` a moral
    consideration does not mean that it does not become a moral consideration in light of the
    desires that almost all real people have. Yes, a psychopath need not be concerned about how his
    actions affect others, but most people are not psychopaths and have a strong interest in
    limiting/counteracting antisocial behavior. (We `),je=n("em"),Vo=a("should"),Zo=a(` take action to mitigate the
    effects of other people’s actions based on desires that are antithetical to our own.) Altruistic
    behavior is totally justifiable under axiom M. In fact, I stand strongly behind the notion that
    nurturing altruistic values is a good path toward Getting What You Want.`),dt=m(),$=n("h2"),ea=a("Feasible and unfeasible desires"),ft=m(),y=n("p"),ta=a("Not all desires are equally feasible. It may be, in this moment, that I "),Ae=n("em"),oa=a("really"),aa=a(`,
    `),Le=n("em"),ia=a("really"),na=a(`, to the exclusion of all other desires, want to have a pet dragon
    (fire-breathing and all). If that is the case, then I should take actions that will make that
    desire a reality. The problem is that, given my current understanding of the way the world
    works, there is really nothing I can do that will make it so I can have a pet dragon. That
    desire is `),He=n("em"),sa=a("unfeasible"),la=a(`. The fact that it is unfeasible doesn’t mean that I shouldn’t try
    to make it happen, if it is truly the only thing I want; it just means that I will almost
    certainly fail at Getting What I Want. If, right now, I want my future self to Get What He
    Wants, then I should take steps to make it less likely for my future self to have unfeasible
    desires.`),yt=m(),A=n("p"),ra=a(`In general, desires lie on a spectrum of feasibility. By this I mean that even the best attempt
    to satisfy a desire is never completely assured to succeed. This suggests that we need some way
    of dealing with the inherent uncertainty associated with the outcomes of our actions when
    considering which action to take. Short of constructing a continuous utility function and
    choosing the action with the highest expected utility (which, as I see it, does not match the
    way that human psychology actually works), I think a reasonable way to proceed is to consider
    your options, make your best estimate of their outcomes, then choose the action with the outcome
    that you prefer. We can think of the idea of feasibility by recognizing that outcomes are
    `),Oe=n("em"),ha=a("stochastic"),ma=a(`, which is to say that a given course of action will not with 100% certainty
    lead to a particular outcome. It may be the case, for example, that action x has a 70%
    probability of resulting in outcome y and a 30% chance of resulting in outcome z (with y and z
    being mutually exclusive). When the results of actions are uncertain in this way, you need to
    consider the entire stochastic space of possible outcomes when deciding which alternative you
    prefer.`),pt=m(),V=n("p"),ua=a(`Let’s take the lottery as an example, assuming that it costs $1 to buy a lottery ticket, that the
    winnings are $1 million, and the probability of winning is one in ten million. When making the
    decision of whether to buy a lottery ticket, the alternatives I choose from are:`),wt=m(),L=n("ul"),Be=n("li"),ca=a(`Action: Buy a ticket. Outcomes: 100% probability of losing $1 and 0.00001% probability of
        winning $1 million.`),da=m(),Se=n("li"),fa=a(`Action: Don’t buy a ticket. Outcomes: 0% probability of losing $1 and 0% probability of
        winning $1 million.`),gt=m(),Z=n("p"),ya=a(`You should choose whichever outcome set you want more. (Minute probabilities like 1/10,000,000
    are, of course, very difficult to think about, but that’s a separate issue.) Since the
    probability of winning $1 million in the lottery is so low, we could call the desire to do so
    unfeasible, but if the desire is strong enough to offset the low probability, it might still be
    the right thing to do.`),bt=m(),N=n("h2"),pa=a("Takeaways"),vt=m(),ee=n("p"),wa=a(`As I’ve already stated, this way of thinking still needs some fleshing-out, and I don’t think it
    is the only reasonable way to approach things. That said, if we do accept axiom M (as I am
    inclined to do) there are at least a few practical takeaways from the above discussion:`),It=m(),I=n("ul"),Ye=n("li"),ga=a(`Allowing short-term desires to have too much mental space is a good way to frustrate your
        future self. If self-consistency and the well-being of your future self is important to you,
        making sure that long-term desires stay in your mental space when you make decisions will
        help keep you from undermining your own self-interest.`),ba=m(),$e=n("li"),va=a(`Don’t worry too much about whether what other people are doing is right or wrong. If you
        don’t like what they’re doing, worry instead about what you can do about it.`),Ia=m(),Ne=n("li"),ka=a(`Don’t chase after impossible wants. Develop the ability to properly quantify uncertainty and
        predict likely outcomes.`),this.h()},l(e){_=s(e,"H1",{});var r=l(_);qt=i(r,"Diving deeper with Axiom M"),r.forEach(t),Xe=u(e),W=s(e,"P",{});var kt=l(W);Pt=i(kt,"I "),D=s(kt,"A",{href:!0});var Wa=l(D);jt=i(Wa,"earlier"),Wa.forEach(t),At=i(kt,` made some
    rather dubitable statements that warrant a bit more careful consideration. Here’s the relevant
    text:`),kt.forEach(t),ze=u(e),X=s(e,"BLOCKQUOTE",{});var Ma=l(X);z=s(Ma,"P",{});var Ea=l(z);se=s(Ea,"EM",{});var Ga=l(se);Lt=i(Ga,"People ought to act in a way that will get them what they want."),Ga.forEach(t),Ht=i(Ea,"…"),Ea.forEach(t),Ma.forEach(t),Fe=u(e),F=s(e,"BLOCKQUOTE",{});var Ta=l(F);le=s(Ta,"P",{});var qa=l(le);Ot=i(qa,`Note that I am not suggesting that we should blindly follow whatever whims flutter across our
        minds. What I am suggesting is that we should endeavor to put ourselves in states that we
        prefer to all other feasible options. Giving precedence to transitory wants is a bad
        strategy for Getting What You Want since those desires are likely to change quickly, making
        them difficult to satisfy in any meaningful way. (As a general maxim, an important part of
        the project of Getting What You Want is wanting the right things in the first place.)`),qa.forEach(t),Ta.forEach(t),Ue=u(e),U=s(e,"BLOCKQUOTE",{});var Pa=l(U);re=s(Pa,"P",{});var ja=l(re);Bt=i(ja,`…it’s helpful to have desires that are actually achievable. It may even be worthwhile to
        intentionally train yourself to desire things that are more feasible.`),ja.forEach(t),Pa.forEach(t),Ce=u(e),M=s(e,"P",{});var Et=l(M);St=i(Et,`For sake of notational simplicity, let’s call the statement “I should act in a way that gets me
    what I want” `),he=s(Et,"EM",{});var Aa=l(he);Yt=i(Aa,"axiom M"),Aa.forEach(t),$t=i(Et,` (’cause it’s Mckay’s Moral axiom). In the rationalist manifesto, I
    made the claim that axiom M is the only sensible moral axiom. That was perhaps too strong of a
    statement – I’m open to the idea that there may be other moral axioms that I could get behind; I
    just haven’t encountered any yet. However, it’s clear from the quotations I’ve included here
    that there are some details about the deployment of this axiom that need to be sorted out.`),Et.forEach(t),Ke=u(e),B=s(e,"H2",{id:!0});var La=l(B);Nt=i(La,"Why this axiom?"),La.forEach(t),Qe=u(e),x=s(e,"P",{});var De=l(x);Dt=i(De,"Before diving into the weeds, I want to take a bit of space to explain "),me=s(De,"EM",{});var Ha=l(me);Xt=i(Ha,"why"),Ha.forEach(t),zt=i(De,` I like axiom
    M.`),ue=s(De,"SPAN",{id:!0}),l(ue).forEach(t),De.forEach(t),Je=u(e),f=s(e,"P",{});var H=l(f);Ft=i(H,`Typically, when people think about morality, what they have in mind is some sort of universal
    principle or set of laws that govern what we `),ce=s(H,"EM",{});var Oa=l(ce);Ut=i(Oa,"should"),Oa.forEach(t),Ct=i(H,` do. For a lot of people, this moral
    law or principle usually comes with some sort of divine authority; that is, there is some sort
    of godlike force or being that dictates what it means for something to be right or wrong.
    (Typically, X is good `),G=s(H,"IMG",{style:!0,title:!0,src:!0,alt:!0}),Kt=i(H,` God says
    X is good.) The problem that I see with this way of thinking is that it’s not clear to me what
    force these moral laws are actually supposed to have: Sure, maybe God says that I shouldn’t do
    X, but what if I just don’t care? Oh, you say that God will punish me if I do X? Well, what if I
    don’t care about the punishment? What if being able to do X now is more important to me than the
    long-term consequence? What if my nature is such that I would actually prefer hell to heaven? If
    that is the case, saying that I should still do X doesn’t seem to make much practical sense.
    Now, depending on your theology, you might have responses to these objections, but the point
    remains that I don’t `),de=s(H,"EM",{});var Ba=l(de);Qt=i(Ba,"have"),Ba.forEach(t),Jt=i(H,` to care, and that point is enough to spur me to look a little
    bit deeper for a principle to base my idea of morality upon. I’ve focused here on the idea that
    moral laws come from some sort of God entity, but the objections I have can be applied even
    (especially!) if you do not believe in a God in the traditional sense. I’ve focused on the
    “divine command” argument simply because it’s been my observation that people who hold that
    perspective tend to have the most vociferous objections to the kind of relativistic thinking
    entailed by something like axiom M, and I don’t think it should be relevant.`),H.forEach(t),Re=u(e),c=s(e,"P",{});var k=l(c);Rt=i(k,`Let’s jump back a little bit and ask ourselves why the idea of morality should have any force at
    all? Well, if you believe that morality comes from God, and that we should be good in order to
    receive God’s blessings and avoid God’s punishments, then what you are implicitly admitting is
    that your idea of morality has force because you `),fe=s(k,"EM",{});var Sa=l(fe);Vt=i(Sa,"want"),Sa.forEach(t),Zt=i(k,` the positive consequences that
    God can impose and `),ye=s(k,"EM",{});var Ya=l(ye);eo=i(Ya,"don’t want"),Ya.forEach(t),to=i(k,` the negative consequences that God can impose. If you
    believe that morality is simply a feature of objective reality (which, as an aside, I personally
    have a really hard time wrapping my head around, but if you have a good argument for this, I’d
    be happy to listen), then again the only reason why you would pay any attention to that
    particular feature of reality is if some part of you `),pe=s(k,"EM",{});var $a=l(pe);oo=i($a,"wants"),$a.forEach(t),ao=i(k,` to take it seriously. And of
    course, if you think that morality is simply an emergent property of the human psyche, then
    ideas about morality can, without much difficulty, be conceptualized as you `),we=s(k,"EM",{});var Na=l(we);io=i(Na,"wanting"),Na.forEach(t),no=i(k,`
    things that you consider to be “good.”`),k.forEach(t),Ve=u(e),T=s(e,"P",{});var xt=l(T);so=i(xt,`It seems like we should take seriously the idea that our fascination with morality is based upon
    the underlying fact that, well, we `),ge=s(xt,"EM",{});var Da=l(ge);lo=i(Da,"want"),Da.forEach(t),ro=i(xt,` things. We have preferences: we have some idea
    of “good” and “bad,” and we want things that are good and don’t want things that are bad.`),xt.forEach(t),Ze=u(e),C=s(e,"P",{});var Xa=l(C);ho=i(Xa,`I hope I’ve made a good enough case that you can at least see where I’m coming from when I say
    that axiom M is sensible. Note that I am deliberate in my choice of the word “axiom.” I do not
    consider this to be an incontrovertible law of nature, or anything of the sort. It is merely a
    convenient place to start, a postulate that seems to take me in useful directions.`),Xa.forEach(t),et=u(e),q=s(e,"P",{});var _t=l(q);mo=i(_t,`One last note before moving on: The thinking that leads me to axiom M is very similar to the
    thinking that leads to utilitarian theories of morality. In fact, axiom M and its corollaries
    are essentially a flavor of `),K=s(_t,"A",{href:!0});var za=l(K);uo=i(za,`preference
        utilitarianism`),za.forEach(t),co=i(_t,`. I note this to show that I am far from the first person to wander down
    these paths of thought (and certainly not the most precise or eloquent expositor of these
    ideas). My intent here is not to reinvent the wheel, rather to suggest how to put these ideas
    into practice and show why thinking in this way is useful.`),_t.forEach(t),tt=u(e),Q=s(e,"P",{});var Fa=l(Q);fo=i(Fa,"Now, let’s move onward and examine what axiom M actually entails."),Fa.forEach(t),ot=u(e),S=s(e,"H2",{id:!0});var Ua=l(S);yo=i(Ua,"Temporal dependence"),Ua.forEach(t),at=u(e),J=s(e,"P",{});var Ca=l(J);po=i(Ca,`For the sake of philosophical exploration, let’s hop inside my mind as I go through the process
    of making a decision.`),Ca.forEach(t),it=u(e),R=s(e,"P",{});var Ka=l(R);wo=i(Ka,`Let’s suppose that I’m hungry. The decision in question is what to do about that hunger. To
    answer that question, we need to consider what it is I want in this moment. Well, I’m hungry, so
    of course I want to eat something. If I’m hungry enough, that desire could be so great that it
    monopolizes my mental space, making it difficult to substantially desire anything else. In such
    a case – according to our axiom – the thing to do is to satisfy that desire by finding something
    to eat; anything else would not be Getting What I Want.`),Ka.forEach(t),nt=u(e),w=s(e,"P",{});var te=l(w);go=i(te,`Now let’s add a little bit of context. Let’s suppose that, for some reason, the only easily
    available food is a tub of ice cream. Eating that ice cream will satisfy my present hunger, but,
    in the long run, might have negative consequences on my health. (Granted, eating ice cream once
    is probably not going to have noticeable consequences, but for the sake of argument let’s
    suppose that this particular ice cream is abnormally unhealthy. Maybe it’s sweetened with lead
    or something.) Should the health effect of the ice cream be a consideration in my
    decision-making process? Well, if I am at a point of hunger that I literally desire nothing else
    except to satisfy my hunger, then – according to our axiom – no. If my `),be=s(te,"EM",{});var Qa=l(be);bo=i(Qa,"only"),Qa.forEach(t),vo=i(te,` desire is
    to satisfy my hunger, then I `),ve=s(te,"EM",{});var Ja=l(ve);Io=i(Ja,"don’t care"),Ja.forEach(t),ko=i(te,` about the health effects. If I don’t care about
    the health effects, then why should they influence my decision?`),te.forEach(t),st=u(e),g=s(e,"P",{});var oe=l(g);Eo=i(oe,"Under normal circumstances, I "),Ie=s(oe,"EM",{});var Ra=l(Ie);xo=i(Ra,"do"),Ra.forEach(t),_o=i(oe,` care about the health effects of my actions. In fact,
    if I go ahead and eat the ice cream, then after my hunger goes away I will probably be much more
    concerned about lead poisoning than about my temporary aversion of hunger. Does that mean that
    it was not the right thing to do? Well, in the moment I made the decision, it `),ke=s(oe,"EM",{});var Va=l(ke);Wo=i(Va,"was"),Va.forEach(t),Mo=i(oe,` the
    right thing to do, since it was the action that would Get Me What I Wanted. However, after the
    fact, with the desire to not be hungry no longer dominating my consciousness, that decision is
    no longer conducive to what I want. If I could go back in time, retaining the desires I have
    after making the decision the first time, it would no longer be the right thing to do.`),oe.forEach(t),lt=u(e),d=s(e,"P",{});var E=l(d);Go=i(E,`I should note that my thinking about all this implicitly assumes that desires are highly
    temporally dependent. That is, the desires present in my mind at one moment are likely different
    from those present at another moment. The things I want today are at least slightly different
    from the things I want tomorrow. If you prefer to think that your desires are in some sense more
    permanent, either based on a commitment to a more solid sense of personal identity or to a sense
    that preferences held on a subconscious level should be relevant, you will likely arrive at
    slightly different conclusions that the ones I draw here. In any case, my claim is that only the
    things I want `),Ee=s(E,"EM",{});var Za=l(Ee);To=i(Za,"right now"),Za.forEach(t),qo=i(E," are relevant to the decision I am making "),xe=s(E,"EM",{});var ei=l(xe);Po=i(ei,"right now"),ei.forEach(t),jo=i(E,`.
    This does `),_e=s(E,"EM",{});var ti=l(_e);Ao=i(ti,"not"),ti.forEach(t),Lo=i(E,` mean that only short-term considerations are relevant, like in the ice
    cream example. My health, for example, is a long-term consideration that `),We=s(E,"EM",{});var oi=l(We);Ho=i(oi,"is"),oi.forEach(t),Oo=i(E,` relevant in
    a lot of the decisions that I make, but it can only be relevant to a particular decision if I on
    some level actually consciously process that desire while I am making the decision.`),E.forEach(t),rt=u(e),P=s(e,"P",{});var Wt=l(P);Bo=i(Wt,`So what is the right thing for me to do after eating the ice cream and regretting having done so?
    Well, since at the current moment I `),Me=s(Wt,"EM",{});var ai=l(Me);So=i(ai,"do"),ai.forEach(t),Yo=i(Wt,` want to be healthy, and eating poisonous ice
    cream is inimical to my health, one good course of action is to act in a way that makes it less
    likely that I will eat poisonous ice cream again in the future. That could involve obvious steps
    like getting rid of the ice cream, or going and buying some other food. It could also involve
    intentionally retraining my mind so that when I get really hungry again, the desire to eat will
    not push aside all other desires.`),Wt.forEach(t),ht=u(e),j=s(e,"P",{});var Mt=l(j);$o=i(Mt,`One thing I want to point out is that, when making any given decision, short-term desires are not
    inherently more or less important than long-term desires. If my short-term desires are at some
    instant more intense than my long-term desires, I should weight my short-term desires more
    heavily, and the reverse is also true. However, if I am at a juncture where long-term desires
    are relevant, Getting What I Want may involve taking steps to make sure that those long-term
    desires `),Ge=s(Mt,"EM",{});var ii=l(Ge);No=i(ii,"remain"),ii.forEach(t),Do=i(Mt,` relevant. (Being ruled by short-term desires may mean that you Get What
    You Want now at the expense of not Getting What You Want later. If all my desires are short
    term, that doesn’t matter, since I don’t care about what happens to me later.) I do think that
    someone who takes axiom M seriously will take deliberate steps to keep long-term desires at the
    forefront of his or her consideration, if only because allowing short-term desires to be too
    prominent will likely frustrate long-term desires when they do show up.`),Mt.forEach(t),mt=u(e),Y=s(e,"H2",{id:!0});var ni=l(Y);Xo=i(ni,"Spatial dependence"),ni.forEach(t),ut=u(e),b=s(e,"P",{});var ae=l(b);zo=i(ae,`Just like I cannot use axiom M to sensibly make claims about I should want in the future (or
    should have wanted in the past) based on my current desires, it is not particularly meaningful
    to employ axiom M for making claims about what other people should or should not be doing given
    `),Te=s(ae,"EM",{});var si=l(Te);Fo=i(si,"my"),si.forEach(t),Uo=i(ae,` personal preferences. I can say things like, “I don’t like the way Alice acts,” but
    statements like, “Bob should not act that way,” don’t make much sense given the framework I’ve
    tried to set up here, unless what I am implying is that Bob’s actions are not aligned with
    `),qe=s(ae,"EM",{});var li=l(qe);Co=i(li,"his"),li.forEach(t),Ko=i(ae,` preferences. In general, I cannot make statements about what is right for other
    people unless I know what it is they want.`),ae.forEach(t),ct=u(e),v=s(e,"P",{});var ie=l(v);Qo=i(ie,`Another point that I should make is that the degree to which other people are Getting What They
    Want is not inherently relevant in my decision making; if I don’t actually care about other
    people, axiom M does not require me to act to benefit them. That said, I want to state
    emphatically that just because the welfare of others is not `),Pe=s(ie,"EM",{});var ri=l(Pe);Jo=i(ri,"inherently"),ri.forEach(t),Ro=i(ie,` a moral
    consideration does not mean that it does not become a moral consideration in light of the
    desires that almost all real people have. Yes, a psychopath need not be concerned about how his
    actions affect others, but most people are not psychopaths and have a strong interest in
    limiting/counteracting antisocial behavior. (We `),je=s(ie,"EM",{});var hi=l(je);Vo=i(hi,"should"),hi.forEach(t),Zo=i(ie,` take action to mitigate the
    effects of other people’s actions based on desires that are antithetical to our own.) Altruistic
    behavior is totally justifiable under axiom M. In fact, I stand strongly behind the notion that
    nurturing altruistic values is a good path toward Getting What You Want.`),ie.forEach(t),dt=u(e),$=s(e,"H2",{id:!0});var mi=l($);ea=i(mi,"Feasible and unfeasible desires"),mi.forEach(t),ft=u(e),y=s(e,"P",{});var O=l(y);ta=i(O,"Not all desires are equally feasible. It may be, in this moment, that I "),Ae=s(O,"EM",{});var ui=l(Ae);oa=i(ui,"really"),ui.forEach(t),aa=i(O,`,
    `),Le=s(O,"EM",{});var ci=l(Le);ia=i(ci,"really"),ci.forEach(t),na=i(O,`, to the exclusion of all other desires, want to have a pet dragon
    (fire-breathing and all). If that is the case, then I should take actions that will make that
    desire a reality. The problem is that, given my current understanding of the way the world
    works, there is really nothing I can do that will make it so I can have a pet dragon. That
    desire is `),He=s(O,"EM",{});var di=l(He);sa=i(di,"unfeasible"),di.forEach(t),la=i(O,`. The fact that it is unfeasible doesn’t mean that I shouldn’t try
    to make it happen, if it is truly the only thing I want; it just means that I will almost
    certainly fail at Getting What I Want. If, right now, I want my future self to Get What He
    Wants, then I should take steps to make it less likely for my future self to have unfeasible
    desires.`),O.forEach(t),yt=u(e),A=s(e,"P",{});var Gt=l(A);ra=i(Gt,`In general, desires lie on a spectrum of feasibility. By this I mean that even the best attempt
    to satisfy a desire is never completely assured to succeed. This suggests that we need some way
    of dealing with the inherent uncertainty associated with the outcomes of our actions when
    considering which action to take. Short of constructing a continuous utility function and
    choosing the action with the highest expected utility (which, as I see it, does not match the
    way that human psychology actually works), I think a reasonable way to proceed is to consider
    your options, make your best estimate of their outcomes, then choose the action with the outcome
    that you prefer. We can think of the idea of feasibility by recognizing that outcomes are
    `),Oe=s(Gt,"EM",{});var fi=l(Oe);ha=i(fi,"stochastic"),fi.forEach(t),ma=i(Gt,`, which is to say that a given course of action will not with 100% certainty
    lead to a particular outcome. It may be the case, for example, that action x has a 70%
    probability of resulting in outcome y and a 30% chance of resulting in outcome z (with y and z
    being mutually exclusive). When the results of actions are uncertain in this way, you need to
    consider the entire stochastic space of possible outcomes when deciding which alternative you
    prefer.`),Gt.forEach(t),pt=u(e),V=s(e,"P",{});var yi=l(V);ua=i(yi,`Let’s take the lottery as an example, assuming that it costs $1 to buy a lottery ticket, that the
    winnings are $1 million, and the probability of winning is one in ten million. When making the
    decision of whether to buy a lottery ticket, the alternatives I choose from are:`),yi.forEach(t),wt=u(e),L=s(e,"UL",{});var Tt=l(L);Be=s(Tt,"LI",{});var pi=l(Be);ca=i(pi,`Action: Buy a ticket. Outcomes: 100% probability of losing $1 and 0.00001% probability of
        winning $1 million.`),pi.forEach(t),da=u(Tt),Se=s(Tt,"LI",{});var wi=l(Se);fa=i(wi,`Action: Don’t buy a ticket. Outcomes: 0% probability of losing $1 and 0% probability of
        winning $1 million.`),wi.forEach(t),Tt.forEach(t),gt=u(e),Z=s(e,"P",{});var gi=l(Z);ya=i(gi,`You should choose whichever outcome set you want more. (Minute probabilities like 1/10,000,000
    are, of course, very difficult to think about, but that’s a separate issue.) Since the
    probability of winning $1 million in the lottery is so low, we could call the desire to do so
    unfeasible, but if the desire is strong enough to offset the low probability, it might still be
    the right thing to do.`),gi.forEach(t),bt=u(e),N=s(e,"H2",{id:!0});var bi=l(N);pa=i(bi,"Takeaways"),bi.forEach(t),vt=u(e),ee=s(e,"P",{});var vi=l(ee);wa=i(vi,`As I’ve already stated, this way of thinking still needs some fleshing-out, and I don’t think it
    is the only reasonable way to approach things. That said, if we do accept axiom M (as I am
    inclined to do) there are at least a few practical takeaways from the above discussion:`),vi.forEach(t),It=u(e),I=s(e,"UL",{});var ne=l(I);Ye=s(ne,"LI",{});var Ii=l(Ye);ga=i(Ii,`Allowing short-term desires to have too much mental space is a good way to frustrate your
        future self. If self-consistency and the well-being of your future self is important to you,
        making sure that long-term desires stay in your mental space when you make decisions will
        help keep you from undermining your own self-interest.`),Ii.forEach(t),ba=u(ne),$e=s(ne,"LI",{});var ki=l($e);va=i(ki,`Don’t worry too much about whether what other people are doing is right or wrong. If you
        don’t like what they’re doing, worry instead about what you can do about it.`),ki.forEach(t),Ia=u(ne),Ne=s(ne,"LI",{});var Ei=l(Ne);ka=i(Ei,`Don’t chase after impossible wants. Develop the ability to properly quantify uncertainty and
        predict likely outcomes.`),Ei.forEach(t),ne.forEach(t),this.h()},h(){p(D,"href","#"),p(B,"id","why-this-axiom"),p(ue,"id","more-247"),Gi(G,"vertical-align","middle"),p(G,"title","\\Leftrightarrow"),Ti(G.src,_a="https://latex.codecogs.com/png.latex?%5CLeftrightarrow")||p(G,"src",_a),p(G,"alt","\\Leftrightarrow"),p(K,"href","https://en.wikipedia.org/wiki/Preference_utilitarianism"),p(S,"id","temporal-dependence"),p(Y,"id","spatial-dependence"),p($,"id","feasible-and-unfeasible-desires"),p(N,"id","takeaways")},m(e,r){h(e,_,r),o(_,qt),h(e,Xe,r),h(e,W,r),o(W,Pt),o(W,D),o(D,jt),o(W,At),h(e,ze,r),h(e,X,r),o(X,z),o(z,se),o(se,Lt),o(z,Ht),h(e,Fe,r),h(e,F,r),o(F,le),o(le,Ot),h(e,Ue,r),h(e,U,r),o(U,re),o(re,Bt),h(e,Ce,r),h(e,M,r),o(M,St),o(M,he),o(he,Yt),o(M,$t),h(e,Ke,r),h(e,B,r),o(B,Nt),h(e,Qe,r),h(e,x,r),o(x,Dt),o(x,me),o(me,Xt),o(x,zt),o(x,ue),h(e,Je,r),h(e,f,r),o(f,Ft),o(f,ce),o(ce,Ut),o(f,Ct),o(f,G),o(f,Kt),o(f,de),o(de,Qt),o(f,Jt),h(e,Re,r),h(e,c,r),o(c,Rt),o(c,fe),o(fe,Vt),o(c,Zt),o(c,ye),o(ye,eo),o(c,to),o(c,pe),o(pe,oo),o(c,ao),o(c,we),o(we,io),o(c,no),h(e,Ve,r),h(e,T,r),o(T,so),o(T,ge),o(ge,lo),o(T,ro),h(e,Ze,r),h(e,C,r),o(C,ho),h(e,et,r),h(e,q,r),o(q,mo),o(q,K),o(K,uo),o(q,co),h(e,tt,r),h(e,Q,r),o(Q,fo),h(e,ot,r),h(e,S,r),o(S,yo),h(e,at,r),h(e,J,r),o(J,po),h(e,it,r),h(e,R,r),o(R,wo),h(e,nt,r),h(e,w,r),o(w,go),o(w,be),o(be,bo),o(w,vo),o(w,ve),o(ve,Io),o(w,ko),h(e,st,r),h(e,g,r),o(g,Eo),o(g,Ie),o(Ie,xo),o(g,_o),o(g,ke),o(ke,Wo),o(g,Mo),h(e,lt,r),h(e,d,r),o(d,Go),o(d,Ee),o(Ee,To),o(d,qo),o(d,xe),o(xe,Po),o(d,jo),o(d,_e),o(_e,Ao),o(d,Lo),o(d,We),o(We,Ho),o(d,Oo),h(e,rt,r),h(e,P,r),o(P,Bo),o(P,Me),o(Me,So),o(P,Yo),h(e,ht,r),h(e,j,r),o(j,$o),o(j,Ge),o(Ge,No),o(j,Do),h(e,mt,r),h(e,Y,r),o(Y,Xo),h(e,ut,r),h(e,b,r),o(b,zo),o(b,Te),o(Te,Fo),o(b,Uo),o(b,qe),o(qe,Co),o(b,Ko),h(e,ct,r),h(e,v,r),o(v,Qo),o(v,Pe),o(Pe,Jo),o(v,Ro),o(v,je),o(je,Vo),o(v,Zo),h(e,dt,r),h(e,$,r),o($,ea),h(e,ft,r),h(e,y,r),o(y,ta),o(y,Ae),o(Ae,oa),o(y,aa),o(y,Le),o(Le,ia),o(y,na),o(y,He),o(He,sa),o(y,la),h(e,yt,r),h(e,A,r),o(A,ra),o(A,Oe),o(Oe,ha),o(A,ma),h(e,pt,r),h(e,V,r),o(V,ua),h(e,wt,r),h(e,L,r),o(L,Be),o(Be,ca),o(L,da),o(L,Se),o(Se,fa),h(e,gt,r),h(e,Z,r),o(Z,ya),h(e,bt,r),h(e,N,r),o(N,pa),h(e,vt,r),h(e,ee,r),o(ee,wa),h(e,It,r),h(e,I,r),o(I,Ye),o(Ye,ga),o(I,ba),o(I,$e),o($e,va),o(I,Ia),o(I,Ne),o(Ne,ka)},p:xa,i:xa,o:xa,d(e){e&&t(_),e&&t(Xe),e&&t(W),e&&t(ze),e&&t(X),e&&t(Fe),e&&t(F),e&&t(Ue),e&&t(U),e&&t(Ce),e&&t(M),e&&t(Ke),e&&t(B),e&&t(Qe),e&&t(x),e&&t(Je),e&&t(f),e&&t(Re),e&&t(c),e&&t(Ve),e&&t(T),e&&t(Ze),e&&t(C),e&&t(et),e&&t(q),e&&t(tt),e&&t(Q),e&&t(ot),e&&t(S),e&&t(at),e&&t(J),e&&t(it),e&&t(R),e&&t(nt),e&&t(w),e&&t(st),e&&t(g),e&&t(lt),e&&t(d),e&&t(rt),e&&t(P),e&&t(ht),e&&t(j),e&&t(mt),e&&t(Y),e&&t(ut),e&&t(b),e&&t(ct),e&&t(v),e&&t(dt),e&&t($),e&&t(ft),e&&t(y),e&&t(yt),e&&t(A),e&&t(pt),e&&t(V),e&&t(wt),e&&t(L),e&&t(gt),e&&t(Z),e&&t(bt),e&&t(N),e&&t(vt),e&&t(ee),e&&t(It),e&&t(I)}}}class ji extends _i{constructor(_){super(),Wi(this,_,null,qi,Mi,{})}}export{ji as default};
