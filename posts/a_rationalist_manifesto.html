<!DOCTYPE html>
<html>
<head>
    <title>Que vivas bien</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="goodness, truth, and summer rainstorms">
    <meta name="author" content="Mckay D Jensen">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="../css/kube.min.css" />
    <link rel="stylesheet" href="../css/font-awesome.min.css" />
    <link rel="stylesheet" href="../css/custom.css" />
    <link rel="shortcut icon" href="../img/favicon.png" />
    <link href="http://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet" type="text/css">
</head>
<body>
	<!-- Navigation -->
	<div class="main-nav">
		<div class="container">
			<header class="group top-nav">
				<nav class="navbar logo-w navbar-left" >
					<a class="logo" href="../index.html">Que vivas bien</a>
				</nav>
				<div class="navigation-toggle" data-tools="navigation-toggle" data-target="#navbar-1">
				    <span class="logo">Que vivas bien</span>
				</div>
			    <nav id="navbar-1" class="navbar item-nav navbar-right">
				    <ul>
				        <li><a href="../index.html">Home</a></li>
				        <li><a href="../about.html">About</a></li>
				        <li><a href="../archive.html">Archive</a></li>
				        <!--<li><a href="#">Contact</a></li>-->
				    </ul>
				</nav>
			</header>
		</div>
	</div>

	<!-- Introduction -->
	<div class="intro">
		<div class="container">
			<div class="units-row">
			    <div class="unit-100">
			    	<p id="tagline" class="p-intro">goodness, truth, and summer rainstorms</p>
			    </div>
			</div>
		</div>
	</div>

	<!-- Content -->
	<div class="content">
		<div class="container">
			<!-- Post -->
			<div class="post">
				<!-- Heading -->
				<h1>A rationalist manifesto</h1>

				<div class="in-content">
  		    <h2 id="the-fundamental-question">The fundamental question</h2>
          <p>The fundamental question of morality is (according to me), “How should I act?” The mass of text that you’re reading right now is my most recent attempt to answer that question in some level of detail. The premise from which I want to approach it is the following:</p>
          <p><em>People ought to act in a way that will get them what they want.</em></p>
          <p>That should seem self-evident. In fact, I am of the opinion that this is the only sensible moral axiom. Why do people, as conscious entities, do anything at all? Because they want things; their desires are the guiding forces for their conscious behaviors. Therefore, in asking yourself if you <em>should</em> do something, it seems quite sensible to simply ask if you <em>want</em> the consequences of the action in question.</p>
          <p>Note that I am not suggesting that we should blindly follow whatever whims flutter across our minds. What I am suggesting is that we should endeavor to put ourselves in states that we prefer to all other feasible options. Giving precedence to transitory wants is a bad strategy for Getting What You Want since those desires are likely to change quickly, making them difficult to satisfy in any meaningful way. (As a general maxim, an important part of the project of Getting What You Want is wanting the right things in the first place.)</p>
          <p>Note also that I am <em>not</em> suggesting that people should behave selfishly, without regard for the welfare/desires of others. Desires to benefit, support, and live in community with others (which almost all humans have) are valid, and typically intense, desires. A person with a normal sense of empathy who is trying to Get What He Wants will take into account the fact that <em>he</em> wants other people to get what <em>they</em> want.</p>
          <h2 id="why-rationality">Why rationality?</h2>
          <p>Given the premise that we ought to act in way that gets us what we want, the question we need to address is how to go about doing that on a practical level. There are a few considerations that I think are important as far as this goes:</p>
          <ul>
          <li>As indicated previously, it’s helpful to have desires that are actually achievable. It may even be worthwhile to intentionally train yourself to desire things that are more feasible.</li>
          <li>It is important to actually be aware of what it is you want. What are your core values? What does your life need to look like for you to feel satisfied? What does your <em>eudaimonia</em> look like?</li>
          <li>Similarly, you need to be good at comparing possible outcomes and determining which you would prefer.</li>
          <li>A knowledge of how the real world works is essential. If you want to achieve your goals, you need to be able to accurately predict the outcomes of your actions. It is important to be able to recognize untrue ideas (and stop believing them), since believing things that are not true will cause you to make errors in determining what is feasible and what the outcomes of your actions will be.</li>
          </ul>
          <p>My claim is that the tools of rational thinking are critical for doing these things as well as possible. We are all equipped with some basic intuitions for establishing priorities, discerning between good and bad options, understanding nature, and arriving at the truth. However, these intuitions are in many cases woefully deficient. (Just take a look at some of the work of <a href="https://en.wikipedia.org/wiki/Daniel_Kahneman">Daniel Kahneman</a> and <a href="https://en.wikipedia.org/wiki/Amos_Tversky">Amos Tversky</a>.)</p>
          <p>I do want to make clear that intuition will <em>generally</em> point you toward what you want, at least when you’re dealing with familiar situations that don’t require analysis of complicated or abstract information. Intuitive heuristics and biases likely evolved to allow humans to save mental effort in processing information and making decisions; without relying to some extent on instinctive mental responses, making everyday decisions would probably be too mentally taxing. However, there are many situations in which <a href="https://alexvermeer.com/hidden-side-of-your-brain/">system 1 thinking</a> will lead to a suboptimal, or simply <em>wrong</em>, conclusion or decision. What is needed, then, is the ability to recognize when more careful thinking is required and the ability to actually think carefully, avoiding common pitfalls, in such cases. These abilities are precisely the <a title="Great resources above and beyond the content of this essay" href="https://www.lesswrong.com/rationality">things that a competent rationalist seeks to develop</a>. Improving your ability to think and behave rationally will, therefore, help you fill in important holes in your capacity to Get What You Want.<span id="more-236"></span></p>
          <h2 id="rational-evaluation-of-evidence">Rational evaluation of evidence</h2>
          <p>One area where rationality is particularly important is in the process of gathering and evaluating evidence to determine the likelihood of ideas being true or the probability of courses of action having a given outcome. Once we move beyond simple propositions (“My hand will not go straight through when I touch a solid wall.”) it can become <em>very</em>, <em>very</em> difficult to do this, and intuition will be dangerously unreliable.</p>
          <h3 id="science">Science</h3>
          <p>Modern science has been successful mainly because the scientific method is specifically designed to avoid the pitfalls of irrational thinking. The following are some important aspects of good scientific procedure that are generalizable to any context where evidence is being evaluated:</p>
          <ul>
          <li>Reproducibility: If something does not happen consistently, it may just have been a probabilistic fluke. If you repeat an experiment enough times, it will probably eventually give you the conclusion you want (and <a href="https://en.wikipedia.org/wiki/Confirmation_bias">confirmation bias</a> can further muddy the waters). Be very wary of anecdotal evidence.</li>
          <li>Care about causality: Correlation does not imply causation. There are ways to determine whether a relationship is causal (optimally, carefully controlled randomized experiments), but you should generally be careful about assuming what causes what.</li>
          <li>Procedure: Good experiments plan their procedure and methods of data analysis beforehand since it is very easy to fall into the trap of continuing to modify procedures or try new analyses until some sort of significant result comes out.</li>
          <li>Emphasis on disproving hypotheses: Evaluation of scientific evidence is typically couched in terms of “rejecting the null hypothesis,” i.e.&nbsp;a given hypothesis is assumed to be reasonable if repeated experimentation fails to show that the hypothesis is unlikely to be true. No matter how convincing an idea might be, it is always possible that future evidence might disprove it. We can only confidently accept a hypothesis if we’ve tested all other alternative hypotheses shown them to be less likely. (More about this in the “Look for alternate hypotheses” section below)</li>
          </ul>
          <p>Remember that bad science does exist. Just because something ostensibly follows the scientific method doesn’t necessarily mean it is trustworthy. It’s best to be at least scientifically literate enough to be able to read and evaluate original scientific research so you can determine when methodology is sloppy or otherwise questionable.</p>
          <h3 id="bayesian-thinking">Bayesian thinking</h3>
          <p>Collecting evidence from multiple sources and sensibly integrating that evidence to inform your judgement of the probability of a given claim being true is something that is best approached carefully and rationally; our intuitions about probability tend to be crude at best.</p>
          <p>One framework for thinking that can help a lot with this is <a title="Wikipedia is God." href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ rule</a>, an important theorem from probability theory:</p>
          <p><img class=" aligncenter" style="vertical-align:middle;" title="P(A|B) = \frac{P(B|A)P(A)}{P(B)}" src="https://latex.codecogs.com/png.latex?P%28A%7CB%29%20%3D%20%5Cfrac%7BP%28B%7CA%29P%28A%29%7D%7BP%28B%29%7D" alt="P(A|B) = \frac{P(B|A)P(A)}{P(B)}"></p>
          <p>You can read this as, “The probability that A is true given that B is true is equal to the probability that B is true given that A is true, multiplied by the probability of A and divided by the probability of B.”</p>
          <p>This formula can be adapted to provide a procedure for updating your beliefs given new evidence. Suppose we want to estimate the probability of some hypothesis H. We have some prior estimate P(H) of the probability of H, and want to know how to adjust our estimate given new evidence E. Bayes’ rule tells us that</p>
          <p><img class=" aligncenter" style="vertical-align:middle;" title="P(H|E) = \frac{P(E|H)}{P(E)} P(H)." src="https://latex.codecogs.com/png.latex?P%28H%7CE%29%20%3D%20%5Cfrac%7BP%28E%7CH%29%7D%7BP%28E%29%7D%20P%28H%29." alt="P(H|E) = \frac{P(E|H)}{P(E)} P(H)."></p>
          <p>That is, when we observe new evidence E, we should update our estimate of the probability of H by multiplying our previous estimate by the ratio P(E|H) / P(E), the probability of seeing that evidence given that the hypothesis is true divided by the probability of seeing that evidence at all. As we continue to gather new evidence, our estimates of P(H) will converge to the true probability of H being true.</p>
          <p>Let’s take a look at an example to see how this can be used practically:</p>
          <p>Suppose that I live in a place where it rains on about half of all days, and I want to figure out the probability that it is going to rain today. I can use the baseline likelihood of 50% as my prior probability of it raining, and then update my estimate based on further evidence. Let’s say that I wake up and see that it is a cloudy day. How should this impact my estimate of the probability of rain? My prior probability P(H) is 0.5. Let’s suppose that it tends to rain on about 3/4 of cloudy days (thus P(E|H) = 0.75), and that about 60% of days are cloudy (thus P(E) = 0.6). Therefore, I should update my estimate as</p>
          <p><img class=" aligncenter" style="vertical-align:middle;" title="P(H|E) = \frac{0.75}{0.6} \times 0.5 = 0.625." src="https://latex.codecogs.com/png.latex?P%28H%7CE%29%20%3D%20%5Cfrac%7B0.75%7D%7B0.6%7D%20%5Ctimes%200.5%20%3D%200.625." alt="P(H|E) = \frac{0.75}{0.6} \times 0.5 = 0.625."></p>
          <p>My observation that it is cloudy should therefore slightly increase my expectation that it will rain today. Now let’s suppose that I make it to midday and it still has not rained. If I know that on days that it rains, the rain starts before noon about 80% of the time, and that it rains before noon on 2 out of every 5 days, I can again update my estimate of the probability that it will rain: the probability that it does <em>not</em> rain before noon given that it will rain sometime during the day is 0.2, and the marginal probability of not having rain before noon is 0.6, so my new estimate should be</p>
          <p><img class=" aligncenter" style="vertical-align:middle;" title="\frac{0.2}{0.6} \times 0.625 \approx 0.208." src="https://latex.codecogs.com/png.latex?%5Cfrac%7B0.2%7D%7B0.6%7D%20%5Ctimes%200.625%20%5Capprox%200.208." alt="\frac{0.2}{0.6} \times 0.625 \approx 0.208."></p>
          <p>Not seeing rain before noon is (given the hypothetical assumptions I made here) strong evidence that it will not rain today.</p>
          <h3 id="look-for-alternate-hypotheses">Look for alternate hypotheses</h3>
          <p>One pitfall that people tend to fall into when evaluating evidence is that they only consider evidence that <em>supports</em> their preconceived ideas and fail to look for evidence that could possibly make those ideas more doubtful. To see why this is problematic, consider the following simple example:</p>
          <p>Let’s suppose I have in mind a function f that takes any number and returns True if that number follows a certain rule and False otherwise. Your objective is to give me numbers that you could use to determine what f’s rule is. Suppose that you start by giving me the number 2, and that I tell you that f(2) = True. You might guess that the rule is that the number must be even, and test that by giving me the number 4. I respond that f(4) = True, which supports your hypothesis. Just to be sure, you give me one more number, 10, to which I respond that f(10) = True. The even numbers idea is looking pretty promising, right? Wrong.</p>
          <p>Let’s now suppose that you remembered my advice about looking for evidence that could prove your ideas wrong, and decided to test your even numbers hypothesis by giving me the number 3. I tell you that f(3) = True, and you know right away that the even numbers idea is incorrect. You could have kept guessing even numbers for as long as you pleased, accumulating “evidence” to support your idea, but you would have been wasting your time and, frankly, fooling yourself. There are any number of hypotheses that, if true, would cause you to observe that f(2) = f(4) = f(10) = True. The rule could be, for example, that any number less than 100 will pass, or that any positive number will pass, or any integer, or even any number whatsoever. You can’t know that those hypotheses are not true until you come up with an experiment to disprove them.</p>
          <p>This applies to much more than just silly number games, of course. In our example about whether it will rain, my estimate of the probability of rain would have been way off if I had failed to take into account the evidence that it had not rained before midday. Failing to look for, or pay attention to, contradictory evidence is especially common when people do not <em>want</em> their ideas to be wrong. This is one of the reasons why it is extremely difficult to change someone’s mind about religious or political dogmas. In general, you will want to keep in mind two questions to reduce your propensity to fall into this trap:</p>
          <ol type="1">
          <li>How likely is it that I would see the evidence I have if my favored hypothesis were not true? If that likelihood is high, the evidence you have so far is not actually strong support of your favored hypothesis. (Think about why this makes sense in the context of Bayesian updating.)</li>
          <li>What is a test I could carry out that could potentially disprove my favored hypothesis?</li>
          </ol>
          <p>A committed rationalist must not be afraid of ugly truths.</p>
          <h3 id="optimism">Optimism</h3>
          <p>Most people are <a href="https://www.nber.org/papers/w24369.pdf">overly optimistic</a> – we overestimate the likelihood of positive events, underestimate the likelihood of negative ones, and are <a href="https://www.kurzweilai.net/brain-imaging-reveals-why-we-remain-optimistic-in-the-face-of-reality?utm_source=KurzweilAI+Daily+Newsletter&amp;utm_campaign=5ea164a0e2-UA-946742-1&amp;utm_medium=email">bad about updating our estimates</a> in the face of new evidence. This is irrational, and thus potentially inimical to our quest of Getting What We Want. However, this is a bias I am not inclined to push back against with much force, since optimism is generally associated with <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2894461/">positive mental health outcomes</a>. To some extent, being more realistic can be counterproductive, since reality is rarely as nice as we might like it to be, and focusing too much on the negative aspects of life can be discouraging. It may be best to figuratively shoot for the moon rather than being too depressed to shoot for anything at all.</p>
          <p>That said, there are scenarios in which it is important not to be too optimistic. One example is the well-documented <a href="https://en.wikipedia.org/wiki/Planning_fallacy">planning fallacy</a>, where people consistently underestimate the amount of time and resources it will take to complete a project, even when they know that they have underbudgeted in the past. There are cases in which failing to plan properly is simply not acceptable (for example, when there is a hard deadline); in those cases, you can combat the planning fallacy by asking yourself how much time/effort/money was required to complete similar projects in the past and assume that your costs will be about the same this time around. In general, if you need to push back against your optimistic biases, comparing the situation you’re in to ones you’ve been in in the past or that people similar to you have been in and assuming that your case is no different is a good way to get a reasonable starting estimate (which you can then refine with a bit of Bayesian updating). As a (not unrealistic) example, if I want to know what grad school I can realistically get into, I should start by finding out where students from my university with similar grades and test scores have gone for grad school, <em>not</em> by blindly assuming that I’m Harvard material.</p>
          <h2 id="rational-measurement-of-value">Rational measurement of value</h2>
          <p>Being able to rationally evaluate evidence and predict likely outcomes is only one part of the toolkit needed to effectively Get What You Want. You also need to be able to accurately measure to what degree various alternatives satisfy your own preferences.</p>
          <p>One way to do this is to think about your decisions as utility maximization problems, that is, to assign a common currency of value to all possible outcomes and choose the decision that will lead you to the highest-valued outcome. This may seem like a bizarre way to go about making decisions, but it is in fact much in vogue for a number of applications (notably and explicitly so, in <a title="Wikipedia is God." href="https://en.wikipedia.org/wiki/Utility_maximization_problem">microeconomics</a>, but also in things like <a href="https://en.wikipedia.org/wiki/Cost%E2%80%93utility_analysis">cost-effectiveness analyses</a> for health interventions).</p>
          <p>The basic idea is to construct a utility function, which we’ll U, which takes options from the set of all possible options and maps them to the real numbers so that given two options x and y, if x is preferred to y, then U(x) is greater than U(y). In mathspeak,</p>
          <p><img class=" aligncenter" style="vertical-align:middle;" title="x \succsim y \Leftrightarrow U(x) \geq U(y)" src="https://latex.codecogs.com/png.latex?x%20%5Csuccsim%20y%20%5CLeftrightarrow%20U%28x%29%20%5Cgeq%20U%28y%29" alt="x \succsim y \Leftrightarrow U(x) \geq U(y)"></p>
          <p>Maximizing U is therefore equivalent to choosing an option that is preferred to all other options. (For more information about the mathematical theory behind this kind of thinking, see <a href="https://quevivasbien.wordpress.com/2019/07/12/formalizing-the-idea-of-preference/">this post</a> I wrote about the subject.)</p>
          <p>It’s not unreasonable to do this with quantifiable goods (as is done when modelling market dynamics in economics), but things get tricky when you try fitting this concept around preferences more generally. It is probably not actually practical to attempt to explicitly define a utility function for day-to-day decision-making, although the idea is still useful to keep in mind, and I do have some tentative ideas that might make that project more tractable:</p>
          <p>I was recently introduced to the idea of using pairwise comparisons to get <a href="https://bradcarmack.blogspot.com/2014/12/the-ginormous-stackrank-of-human.html">numerical scores for human experiences</a> in much the same way that chess players are ranked using the <a title="Wikipedia is God." href="https://en.wikipedia.org/wiki/Chess_rating_system#Elo_rating_system">Elo rating system</a>. This method of scoring experiences is attractive because it allows us to build a sort of utility function in a way that is intuitively quite natural (pairwise comparisons are cognitively much easier to consider than just assigning a numerical value to an experience <em>desde cero</em>).</p>
          <p>This sort of approach could also allow a person to get an idea of how she responds to certain “distance” factors. For example, you could, with mathematical precision, answer questions like:</p>
          <ul>
          <li>How do you discount value based on…
          <ul>
          <li>time? (e.g.&nbsp;something happening now vs.&nbsp;tomorrow vs.&nbsp;ten years from now)</li>
          <li>distance? (e.g.&nbsp;something happening to you vs.&nbsp;a family member vs.&nbsp;someone on the other side of the world you will never meet)</li>
          </ul>
          </li>
          <li>How risk averse are you?
          <ul>
          <li>Is the negative value of losing something more than the positive value of gaining the same thing? By how much?</li>
          <li>How do you discount value for uncertain events?</li>
          </ul>
          </li>
          </ul>
          <p>Knowing this type of thing is, of course, useful for Getting What You Want because it allows you to both have a more concrete idea of what you want and how various factors will effect how much you value given outcomes. It may also allow you make at least certain dimensions of your decision space continuous and hopefully convex, which, at least from a mathematical perspective, is very attractive.</p>
          <p>I do actually intend to eventually write a computer program to build utility functions in this way (and ideally use it for some economic research), so let me know if you’re interested in the idea.</p>
          <h2 id="conclusion">Conclusion</h2>
          <p>Thanks for reading this far! To reward your persistence, here’s a summary of the main points I wanted to convey with this “manifesto:”</p>
          <ul>
            <li>It is sensible to approach decisions by asking, “Will this get me what I want?”</li>
            <li>Being able to think rationally (and knowing when rational thinking is necessary) is helpful for Getting What You Want.</li>
            <li>The scientific method is a good way to hone in on true/reliable ideas, but only when followed carefully.</li>
            <li>Bayesian updating is an indispensable tool for processing new evidence.</li>
            <li>It is important to look for evidence that contradicts your ideas; otherwise, you risk fooling yourself.</li>
            <li>When it’s important to be right, err on the side of pessimism.</li>
            <li>The idea of utility maximization is helpful to keep in mind.</li>
          </ul>
				</div>

				<div class="foot-post">
					<!-- place footer here -->
				</div>
			</div>
			<!-- /post -->

		</div>
	</div>

  <footer>
		<div class="container">
			<p class="text-centered foot-cp">
	    		<a href="../about.html">created by mckay jensen</a>
	    </p>
      <p class="text-centered" style="font-size: 24px;">
        <a href="mailto:jensenm@uchicago.edu"><i class="fa fa-send"></i></a>
        <a href="https://github.com/quevivasbien"><i class="fa fa-github"></i></a>
        <a href="https://www.linkedin.com/in/mckaydjensen/"><i class="fa fa-linkedin"></i></a>
      </p>
		</div>
	</footer>

	<!-- Javascript -->
	<script src="../js/jquery.min.js"></script>
    <script src="../js/kube.min.js"></script>
	<script src="../js/beautiful.js"></script>
</body>
</html>
