<!DOCTYPE html>
<html>
<head>
    <title>Que vivas bien</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="goodness, truth, and summer rainstorms">
    <meta name="author" content="Mckay D Jensen">
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link rel="stylesheet" href="../css/kube.min.css" />
    <link rel="stylesheet" href="../css/font-awesome.min.css" />
    <link rel="stylesheet" href="../css/custom.css" />
    <link rel="shortcut icon" href="../img/favicon.png" />
    <link href="http://fonts.googleapis.com/css?family=Lato:400,700" rel="stylesheet" type="text/css">
    <link href="http://fonts.googleapis.com/css?family=Open+Sans:400,700" rel="stylesheet" type="text/css">
</head>
<body>
	<!-- Navigation -->
	<div class="main-nav">
		<div class="container">
			<header class="group top-nav">
				<nav class="navbar logo-w navbar-left" >
					<a class="logo" href="../index.html">Que vivas bien</a>
				</nav>
				<div class="navigation-toggle" data-tools="navigation-toggle" data-target="#navbar-1">
				    <span class="logo">Que vivas bien</span>
				</div>
			    <nav id="navbar-1" class="navbar item-nav navbar-right">
				    <ul>
				        <li><a href="../index.html">Home</a></li>
				        <li><a href="../about.html">About</a></li>
				        <li><a href="../archive.html">Archive</a></li>
				        <!--<li><a href="#">Contact</a></li>-->
				    </ul>
				</nav>
			</header>
		</div>
	</div>

	<!-- Introduction -->
	<div class="intro">
		<div class="container">
			<div class="units-row">
			    <div class="unit-100">
			    	<p id="tagline" class="p-intro">goodness, truth, and summer rainstorms</p>
			    </div>
			</div>
		</div>
	</div>

	<!-- Content -->
	<div class="content">
		<div class="container">
			<!-- Post -->
			<div class="post">
				<!-- Heading -->
				<h1 id="modelling-some-failure-modes-of-ai">Modelling some failure modes of AI</h1>

				<div class="in-content">

                    <p>In this section, we present some potential ways in which AI may cause harm and show how we might fit these problems into <a href="./ai-governance-series-model-intro.html">our model</a>.</p>
					<p>The harms discussed here are all near-term harms in the sense that they are currently occurring to some extent and may worsen within the next few decades. These may not be the most significant harms in the big picture, but they may translate to bigger problems in the long term, and they’re arguably easier to conceptualize so can be good for tuning our thinking.</p>
					<h2 id="some-potential-harms">Some potential harms</h2>
					<p>We’ll start by discussing some broad classes of failure modes. In the next subsection, we’ll take a couple of more specific examples and show how they can be modelled.</p>
					<h3 id="information-degradation">Information degradation</h3>
					<p>One broad way that AI may cause harm in the near term is by degrading the quality of information (particularly in online media) in a way that is harmful to democratic processes or pushes people to make suboptimal consumption decisions (in terms of what they purchase and how they spend their time &amp; attention).</p>
					<p>This sort of thing is particularly visible in the case of online ad &amp; content recommendation. Major internet companies make money by collecting information about their users and using that information to give users targeted ads. This can become problematic when (1) use of information constitutes a breach of privacy, and (2) ad recommendation systems increase click-through rates not just by providing relevant ads but by manipulating users’ preferences and/or attention. Ad recommendation is closely linked with the issue of content recommendation – also driven by AI: since more engagement with content = more ads, and has been associated with “filter bubbles,” spreading of false information, proliferation of inflammatory content, etc. It seems reasonable that more capable AI may increase these harms if implementers don’t take sufficient care.</p>
					<h3 id="labor-market-effects">Labor market effects</h3>
					<p>If AI replaces human labor faster than it increases productivity, it can constitute transfer of wealth away from laborers, which may be socially undesirable. Even if labor’s absolute share of income is not reduced, automation causes holders of capital to have a larger <em>relative</em> share of wealth (which some people may find problematic) and tends to cause at least temporary unemployment as the labor market adjusts. Beyond their economic undesirability, these changes may also be politically destabilizing. AI may eventually obviate all human labor, which would necessitate <a href="https://www.dropbox.com/s/ynfbi7280tedflp/NonexistentFuture.pdf?dl=0">a drastic change in economic thinking</a>.</p>
					<h3 id="dependence-for-critical-systems">Dependence for critical systems</h3>
					<p>If AI is used in critical systems (like the electricity grid, production/shipping supply chains, or military infrastructure), they may become vulnerable to disasters or hacking that compromise the AI’s integrity or force it to go offline. It might be prohibitively difficult to ensure that AI systems don’t have serious security vulnerabilities and won’t sometimes behave unpredictibly, making this potentially more of a concern than with traditional computer systems (which is already a big concern).</p>
					<h3 id="surveillance">Surveillance</h3>
					<p>AI could enable close surveillance, which, among other things, would be a boon to totalitarian governments.</p>
					<h3 id="bad-actors">Bad actors</h3>
					<p>AI can allow whoever has money to have outsized influences on others since an AI trained for a given task can be replicated many times, constrained only by the cost of hardware. This isn’t inherently a problem but could have very bad consequences if someone with antisocial intentions has enough resources.</p>
					<p>This is clearly concerning insofar as it enables violence: people could buy drones to kill or otherwise harm others, enabling new types of crime, terrorism, and warfare. In the past, people’s capacity for violence has been limited by their own physical strength and their ability to recruit others; in the future, they may be limited only their ability to purchase or build intelligent bots.</p>
					<h2 id="modelling-examples">Modelling examples</h2>
					<p>What follows are some examples of how the model framework can be used for these types of harms. The idea here is not to derive any particular results, but just to give some inspiration about how to adapt <a href="./ai-governance-series-model-intro.html">our model</a> to these scenarios.</p>
					<h3 id="content-recommendation">Content recommendation</h3>
					<p>Let’s assume first that the AI involved is not sufficiently advanced that alignment with the manager is a big concern, so <span class="math inline">\(x = \hat x\)</span>. We’ll let <span class="math inline">\(x\)</span> denote exposure to content (including ads). If the manager/content provider behaves like a monopoly (which is not so unrealistic; e.g., there is not strong competitor to YouTube), then they simply set <span class="math inline">\(t(x) = u(x)\)</span>, (charging consumers / advertisers the maximum they will pay for a given amount of content &amp; advertising) and set <span class="math inline">\(x\)</span> to maximize <span class="math inline">\(u(x) - c(x)\)</span>.</p>
					<p>There are lots of ways we can make this scenario more interesting. For example, suppose a social planner wants to intervene to maximize the users’ welfare. Then the problem is to choose <span class="math inline">\(t\)</span> to maximize <span class="math display">\[u(x) - t(x)\]</span> subject to <span class="math display">\[x = \text{argmax}_{x} \left \{ t(x) - c(x) \right \}\]</span> and <span class="math display">\[t(x) - c(x) \geq 0.\]</span></p>
					<p>We could also assume, for example, that consumers differ in their utilities <span class="math inline">\(u\)</span>, or that multiple, possibly different, content providers are competing for users.</p>
					<p>Now let’s assume that the AI is imperfectly aligned, so it recommends content by achieving a proxy objective <span class="math inline">\(\hat x\)</span> that sometimes differs from the true objective <span class="math inline">\(x\)</span>; i.e., for some <span class="math inline">\(\hat x\)</span>, <span class="math inline">\(\hat x \neq \xi(x)\)</span>. To make this concrete, let’s suppose that <span class="math display">\[\xi(\hat x) = 1 - (x - \alpha)^2,\]</span> for some constant <span class="math inline">\(\alpha\)</span>, which the manager can choose for some cost. This is an inverted parabola with its vertex at <span class="math inline">\(\alpha\)</span>; thus, the true objective is maximized at <span class="math inline">\(\hat x = \alpha\)</span>; the manager’s payoff can be increased either by moving <span class="math inline">\(\hat x\)</span> closer to <span class="math inline">\(\alpha\)</span> (changing the AI’s performance) or moving <span class="math inline">\(\alpha\)</span> closer to a point with lower cost (changing the AI’s alignment). The social planner now chooses <span class="math inline">\(t\)</span> to maximize <span class="math display">\[u(x) - t(x)\]</span> with <span class="math display">\[x = 1 - (\hat x - \alpha)^2\]</span> subject to <span class="math display">\[\hat x, \alpha = \text{argmax}_{\hat x,\ \alpha} \left\{ t(\hat x) - c(\hat x; \alpha) \right\}.\]</span></p>
					<p>Note that in this example, the manager knows how <span class="math inline">\(x\)</span> and <span class="math inline">\(\hat x\)</span> are related, so the “alignment” problem here is really just a transformation of the manager’s cost function. Typically, we worry more about cases where the proxy goal does not match the true goal, and the manager is either unaware of this or how some uncertainty about the relationship between the two. We’ll look at an example of this below.</p>
					<h3 id="automation-of-labor">Automation of labor</h3>
					<p>Let’s suppose that we have some product whose production can be partially automated, with the amount of automation represented here by <span class="math inline">\(x\)</span>.</p>
					<p>We’ll have laborers take the role of the “public” in this example. Taking some inspiration from <a href="https://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.33.2.3">Acemoglu and Restrepo</a>, we know that automation is likely to be beneficial to workers only if it increases productivity enough to offset the displacement effect of automation. Therefore, if we assume that automation has decreasing marginal productivity (so new automation becomes less productive at higher levels), then laborers should have their welfare <span class="math inline">\(u(x)\)</span> be concave in <span class="math inline">\(x\)</span>, reaching a maximum at a point that we’ll call <span class="math inline">\(x^*\)</span> and decreasing thereafter.</p>
					<p>Continuing the assumption of decreasing marginal productivity and assuming also that the manager’s cost of <span class="math inline">\(x\)</span> is not concave, the manager’s <em>net</em> cost <span class="math inline">\(c(x)\)</span> should be convex, with a minimum at a point that we’ll call <span class="math inline">\(x^\dagger\)</span>.</p>
					<p>Consequently, with these assumptions, in the absence of transfers between the laborers and the manager, laborers will prefer <span class="math inline">\(x^*\)</span>, and the manager will prefer <span class="math inline">\(x^\dagger\)</span>. If what we care about is the welfare of the laborers, then we’re probably worried about situations where the manager can choose the level of automation, and <span class="math inline">\(x^\dagger \neq x^*\)</span>. In that case, the laborers (or someone acting in their interest) may be willing to transfer some wealth to the manager in order to shift <span class="math inline">\(x\)</span> – the problem to be solved is to choose <span class="math inline">\(t\)</span> to maximize <span class="math display">\[u(x) - t(x)\]</span> subject to <span class="math display">\[\hat x = \text{argmax}_x\{t(x) - c(x)\}.\]</span> You’ll notice that this is the same problem as in the content recomendation example (although, of course, the variables/functions represent different things here).</p>
					<p>To explore how poor alignment might be a problem in this example, let’s suppose that the manager specifies the AI’s proxy objective as something like, “Automate as much of the production process as possible.” The AI might realize that if it increases its productivity, then the manager will choose a higher level of automation, but it could achieve the same result by simply automating more than the manager wants it to. If the AI does this and the manager is unaware, then the manager will think that they are choosing a level <span class="math inline">\(\hat x\)</span> of automation, but actually <span class="math inline">\(x &gt; \hat x\)</span>. The manager will then choose a level of automation that is higher than the optimum; if <span class="math inline">\(x^\dagger &lt; x^*\)</span>, then this is also bad for the laborers. We could also suppose that the manager knows that the AI will do this, but doesn’t know by how much – let’s say that <span class="math display">\[x = \hat x + \delta\]</span> where <span class="math inline">\(\delta\)</span> is some nonnegative random variable. Then the manager chooses <span class="math inline">\(\hat x\)</span> to maximize the expected value of <span class="math inline">\(t(\hat x + \delta) - c(\hat x + \delta)\)</span> given their prior on <span class="math inline">\(\delta\)</span>. The manager should end up better off than if they were unaware of the AI’s manipulation but will still be worse off than if they knew exactly what input <span class="math inline">\(\hat x\)</span> to choose. The outcome also depends on the information that the laborers have. For example, if the laborers (or their representative that makes transfers for them) have the same information about <span class="math inline">\(\delta\)</span> as the manager, the transfer-choice problem is to choose <span class="math inline">\(t\)</span> to maximize <span class="math display">\[\mathbb E[u(\hat x + \delta)-t(\hat x + \delta) | \mathcal I]\]</span> subject to <span class="math display">\[\hat x = \text{argmax}_{\hat x} \mathbb E[t(\hat x + \delta) - c(\hat x + \delta) | \mathcal I],\]</span> where <span class="math inline">\(\mathcal I\)</span> is the common information about <span class="math inline">\(\delta\)</span>.</p>
					<hr />
					<p>In the <a href="./ai-governance-series-safety-performance.html">next section</a>, we’ll consider a formulation of this model meant to capture a tradeoff between safety and performance.</p>

                </div>

				<div class="foot-post">
					<!-- place footer here -->
				</div>
			</div>
			<!-- /post -->

		</div>
	</div>

  <footer>
		<div class="container">
			<a href="../index.html"><img class="aligncenter" src="../img/logo.png" title="Back to home page"></a>
			<p class="text-centered foot-cp">
	    		<a href="../about.html">created by mckay jensen</a>
	    </p>
      <p class="text-centered" style="font-size: 24px;">
        <a href="mailto:jensenm@uchicago.edu"><i class="fa fa-send"></i></a>
        <a href="https://github.com/quevivasbien"><i class="fa fa-github"></i></a>
        <a href="https://www.linkedin.com/in/mckaydjensen/"><i class="fa fa-linkedin"></i></a>
      </p>
		</div>
	</footer>

	<!-- Javascript -->
	<script src="../js/jquery.min.js"></script>
    <script src="../js/kube.min.js"></script>
	<script src="../js/beautiful.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>
